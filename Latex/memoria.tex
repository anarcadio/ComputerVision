\documentclass[a4paper,openright, 12pt]{book}
\usepackage[spanish]{babel} 
\usepackage[utf8]{inputenc} 
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   
  basicstyle=\footnotesize,       
  breakatwhitespace=false,         
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Python,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=none,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=4,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}




\begin{document}

\begin{titlepage}
\begin{center}
\begin{Huge}
\textsc{Título}
\end{Huge}
\end{center}
\end{titlepage}

\newpage
\mbox{}
\thispagestyle{empty} 


\tableofcontents % indice de contenidos
\newpage
\thispagestyle{empty}


\chapter{Introducción}\label{cap.introduccion}
\pagenumbering{arabic} % para empezar la numeración con números

\section{Motivación}
\chapter{OpenCV}

\section{Introducción}
OpenCV es una librería de código abierto escrita en C y C++ destinada a la visión artificial y al tratamiento de imágenes. Se trata de una librería multiplataforma con versiones para GNU/Linux, Windows, Mac OS y Android y actualmente cuenta con interfaces para Python, Java y MATLAB/OCTAVE. Las últimas versiones incluyen soporte para GPU usando CUDA.
Desarrollada originalmente por Intel, su primera versión alfa se publicó en el año 2000 en el \textit{IEEE Conference on Computer Vision and Pattern Recognition}. OpenCV nació inicialmente como un proyecto para avanzar en las aplicaciones de uso intenso de la CPU y dando gran importancia a las aplicaciones en tiempo real. Hoy en día cuenta con más 2500 algoritmos optimizados que abarcan todo tipo de campos relacionados con la visión artificial.
Estos algortimos pueden ser usados para tareas como: detección y reconocimiento de caras y gestos, identificación objetos, detección de características 2D y 3D, estimación y seguimiento del movimiento, visión estéreo y calibración de la cámara, eliminación los ojos rojos de las fotografías realizadas con flash...
\newline
OpenCV es ampliamente utilizada por todo tipo de empresas (desde grandes empresas como Google, Yahoo, Microsoft, Intel, IBM, Sony, Honda, Toyota a pequeñas empresas), grupos de investigación y organismos gubernamentales y en sectores de todo tipo como: inspección de los productos en las fábricas, seguridad, usos médicos, robótica...
\newline
Como ya hemos dicho, openCV incluye todo tipo de funciones e implementaciones de algoritmos. 
Dichas funciones se pueden dividir en varios grupos:
\begin{itemize}
\item Las relacionadas con el interfaz gráfico: se encargan de crear ventanas, mostrar imágenes por pantalla, registrar las pulsaciones del teclado, y de crear la interfaz de usuario (botones, barras de desplazamiento, etc). Veremos las más importantes en los ejemplos de la sección siguiente.
\item Las estructuras básicas como las matrices y las funciones para trabajar con ellas. En la versión para python esto no es muy importante porque está adaptada para trabajar con arrays de Numpy.
\item Las funciones de procesado de imágenes. Es la parte fundamental de openCV y contiene fucniones de todo tipo: 
\begin{itemize}
\item Filtros de imagen, como desenfoques varios, ampliar o reducir una imagen, calcular el gradiente de una imagen mediante el operador Sobel...
\newline Veremos algunas de estas funciones en los ejemplos.
\item Transformaciones geométricas. Estas funciones realizan diversas tareas de carácter geométrico sobre las imágenes 2D, como calcular la transformación afín que lleva unos puntos a otros, calcular la matriz de una rotación o aplicarle a una imagen la matriz de una transformación.
\item Funciones relacionadas con el cálculo y manejo de histogramas (comparar dos histogramas, normalizar un histograma...).
\item Funciones de estimación de movimiento y detección de características. La más importantes son usadas y explicadas en el capítulo 4.
\end{itemize}
\item Funciones de calibrado de la cámara (para corregir la distorsión provocada por la lente) y de reconstrucción 3D usando visión estéreo (dos cámaras).
\item Funciones para detectar objetos y caras. Destaca la clase \lstinline|CascadeClassifier|.
\item Además, openCV consta de una librería de aprendizaje automático (\textit{machine learning}) que está formada por funciones de clasificación estadística, árboles de decisión, redes neuronales...
\end{itemize}
Por supuesto openCV es una libería enorme y consta de muchísimas más funciones que las aquí mencionadas\newpage
\section{Ejemplos}
A continuación veremos una serie de ejemplos sencillos de uso de openCV con python, similares a los propuestos en el capítulo 2 de \textit{"Learning OpenCV" \cite{oreilly}}
\subsection*{Ejemplo 1}
En primer lugar veamos como mostrar una imagen en una ventana
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo1.py}
Como vemos se trata de un código muy sencillo. Al ser ejecutado con un argumento, carga una imagen y la muestra en una ventana; luego espera hasta que el usuario pulse una tecla.

La función \lstinline|cv2.imread()| es la que se encarga de cargar la imagen desde un archivo.  Esta función toma como argumentos el nombre del archivo que se quiere abrir y además podemos especificar con un segundo parámetro opcional el modo de color en que se carga la imagen.

La función devuelve la imagen (en forma de array de numpy).
\newline
Una vez hemos cargado la imagen, la mostramos en una ventana usando \lstinline|cv2.imshow(winname, image)|, donde \lstinline|winname| es el nombre de la ventana e \lstinline|image| es la imagen que queremos mostrar.
Por último, utilizamos \lstinline|cv2.waitKey(0)| para esperar a que el usuario pulse una tecla. Esta función recibe un único parámetro que indica el tiempo de espera en milisegundos para la pulsación de una tecla. Si este parámetro es menor o igual que cero, esperará indefinidamente.

\newpage

\subsection*{Ejemplo 2}
En el siguiente ejemplo vemos como cargar un archivo de video y mostrarlo en una ventana. El programa termina cuando el video se acaba o cuando el usuario pulsa la tecla ``Esc"
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo2.py}
En este ejemplo hemos hecho uso de la clase VideoCapture.
La función \lstinline|cv2.VideoCapture(filename)| recibe como argumento el nombre de un archivo de video o la ID de un dispositivo de video (en este caso un nombre de archivo) y devuelve un objeto de la clase VideoCapture.
De esta clase tan solo usamos ahora la función \lstinline|cv2.VideoCapture.read()| que toma el siguiente fotograma, lo decodifica y lo devuelve. Además de la imagen, devuelve un booleano, que en el código hemos llamado ``\lstinline|ret|", que es \textit{\lstinline|False|} si ningún fotograma ha sido tomado y \textit{\lstinline|True|} en otro caso.
Como ahora ya tenemos una imagen, para mostrarla por pantalla usamos la misma función que en el ejemplo 1.
Por último, esta vez usamos la función \lstinline|cv2.waitKey| de manera un poco distinta al anterior ejemplo. Como ahora estamos en un bucle, esperamos a la pulsación 33 milisegundos. Si alguna tecla es pulsada, su valor ASCII se almacena y lo comparamos con 27 que es el correspondiente a la tecla ``Esc".

\newpage
\subsection*{Ejemplo 3}
En este ejemplo mejoraremos un poco el reproductor de video que hemos programado en el ejemplo anterior. Se ha añadido una barra de deslizamiento con el instante del video en que nos encontramos, el cual se puede utilizar para avanzar y retroceder hasta donde deseemos.
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo3.py}
Como vemos, en este código hemos introducido unas cuantas funciones nuevas.
En primer lugar, hemos usado varias veces la función \lstinline|cv2.VideoCapture.get(propId)| con distintos argumentos.
Esta función devuelve el valor de distintas propiedades del video con el que estamos trabajando. El argumento es el identificador de la propiedad entre los que destacan: \lstinline|CV_CAP_PROP_POS_FRAMES| (posición del próximo fotograma que va a ser decodificado/capturado), \lstinline|CV_CAP_PROP_FPS| (fotogramas por segundo) y \lstinline|CV_CAP_PROP_FRAME_WIDTH|y \lstinline|CV_CAP_PROP_FRAME_HEIGHT| (anchura y altura de los fotogramas del video respectivamente).
La lista completa de opciones puede ser encontrada fácilmente en la documentacíon de openCV.

También usamos la función \lstinline|cv2.VideoCapture.set(propId, value)|, usada para fijar el valor de alguna de las propiedades del video. Por tanto los valores que puede tomar \lstinline|propId| son los mismos que para la función \lstinline|cv2.VideoCapture.get()| y el argumento \lstinline|value| indica el nuevo valor que tomará la propiedad indicada.

Una vez cargamos el video como ya sabemos y obtenemos el número total de fotogramas con la función \lstinline|get| como acabamos de ver, procedemos a crear una ventana con la función \lstinline|cv2.namedWindow(winname[, flags])|.
Hasta ahora no había hecho falta ya que \lstinline|cv2.imshow| creaba una nueva ventana si no existía una con ese nombre.
Ahora le vamos a añadir algo (una trackbar) a nuestra ventana, por lo que necesitamos que ya esté creada.
Para esto usamos \lstinline|cv2.createTrackbar(trackbarName, windowName, value, count, onChange)|, siendo estos parámetros el nombre de la barra de deslizamiento creada, el nombre de la ventana en la que queremos la barra,una variable que refleja la posición del deslizador, la posición máxima del deslizador (la posición mínima es 0 siempre) y por último una función que será llamada cada vez que el slider cambie de posición.

\newpage

\subsection*{Ejemplo 4}
Ahora vamos a ver un ejemplo de una transformación muy sencilla, vamos a abrir una imagen y a aplicarle un desenfoque gaussiano y a mostrar tanto la imagen original como la resultante por pantalla.
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo4.py}
La única función nueva es 
\lstinline|cv2.GaussianBlur()|, que es la que realiza el desenfoque.




\newpage

\subsection*{Ejemplo 5}
En este ejemplo realizamos otra transformación, en este caso reducimos la imagen a la mitad. Esta tarea la realiza la función \lstinline|cv2.pyrDown|
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo5.py}
Como vemos, en el resto del código no hay nada nuevo, a excepción de la función ya nombrada.
\newpage
\subsection*{Ejemplo 6}
En este ejemplo utilizamos el algoritmo de Canny \cite{canny86} de detección de bordes, usando para ello la función \lstinline|cv2.Canny|
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo6.py}
\newpage 
\subsection*{Ejemplo 7}
Ahora simplemente vamos a combinar los dos ejemplos anteriores, reduciendo dos veces la imagen original con \lstinline|pyrDown| y luego encontrando los bordes con \lstinline|Canny|
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo7.py}
\newpage
\subsection*{Ejemplo 8}
A continuación vemos un ejemplo que muestra la imagen de la \textit{webcam} y utiliza el algoritmo de Canny de detección de bordes.
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo8.py}
Como vemos el código para mostrar una imagen de la \textit{webcam} es exactamente el mismo que el de mostrar un video por pantalla, excepto que a la función \lstinline|cv2.VideoCapture| le pasamos como argumento, en lugar del nombre del archivo, un entero que representa el índice de la cámara (si solo hay una cámara, debe ser 0).
\newpage
\subsection*{Ejemplo 9}
\lstinputlisting[language=Python, frame= single]{ejemplos/ejemplo9.py}
\newpage
\chapter{Otras librerías usadas}
\section{Android}

Android es un sistema operativo desarrollado por Google y orientado a móviles y tablets. Está basado en Linux y es de código abierto. Actualmente se estima que en torno a un 80\% de los dispositívos móviles usan el sistema operativo Android.
Su núcleo está programado en C, pero la aplicaciones y toda la interfaz de usuario se programan en Java.
Este sistema operativo está estructurado en 4 capas:
\begin{description}
  \item[Núcleo Linux] \hfill \\
  Se encarga de las funcionalidades básicas del sistema, como manejo de procesos, de la memoria y de los dispositivos como la cámara, la pantalla, etc.
   Asimismo funciona como una capa de abstracción entre el \textit{hardware} y el resto del \textit{software}.
   
  \item[Librerías y \textit{runtime} de Android] \hfill \\
  Por encima del núcleo, hay una serie de librerías usadas por componentes del sistema, entre ellas destacan Surface Manager, Media Framework, SQLite, WebKit, SGL y Open GL
  \newline
  El runtime de Android proporciona un componente clave,la máquina virtual Dalvik. Cada aplicación Android corre su propio proceso, con su propia instancia de esta máquina virtual.
  Además, el \textit{runtime} de Android proporciona librerías básicas que proporcionan la mayor parte de las funciones del lenguaje de programación Java.
  
  \item[Marco de trabajo(\textit{framework}) de aplicaciones] \hfill \\
  Esta capa proporciona a las aplicaciones muchos servicios en forma de clases de Java 
  
   \item[Aplicaciones] \hfill \\
  En esta capa se encuentran tanto las aplicaciones base como las que instalemos y desarrollemos   \ldots
\end{description}

\subsection*{Estructura de una aplicación Android}
Las aplicaciones Android están formadas por los llamados componentes de aplicación. Hay cuatro tipo de componentes:
\begin{description}
  \item[Activities] \hfill \\
  Representa una única pantalla de la aplicación con su interfaz de usuario.
  \item[Services] \hfill \\
  Son componentes que se ejecutan en segundo plano y que no constan de interfaz gráfica.
  \item[Content providers] \hfill \\
  Se encarga de compartir información entre distintas aplicaciones.
  \item[Broadcast receivers] \hfill \\
  Este componente permite el registro de eventos del sistema
\end{description}
Una vez visto esto veamos por encima la estructura de una aplicación Android.
Dichas aplicaciones vienen en formato APK, que no es más que un fichero comprimido ZIP en el que se encuentran: el código, los recursos, la firma digital y el fichero de manifiesto.
\newline
Los recursos se encuentran en la carpeta \lstinline|res|. Además de las imágenes y otro tipo de contenido que usemos en la aplicación, esta carpeta contiene otra carpeta, llamada \lstinline|layouts|, en la que se encuentran archivos XML que describen las distintas pantallas o vistas de cada aplicación, es decir las interfaces gráficas asociadas a cada actividad.
\newline
El archivo ``\lstinline|manifest.xml|'' es un fichero XML donde se declaran los componentes que conforman la aplicación, así como se indican los permisos necesarios que requiere la aplicación, las funciones de \textit{hardware} o \textit{software} que se necesitan (la cámara, el acelerómetro, los servicios de \textit{bluetooth}...) y algunás características más del programa.

\chapter{Estabilización de imagen}\label{cap.}

\section{Algoritmo básico}
Se trata de un algoritmo básico de estabilización de imagen para eliminar el posible movimiento de agitación la cámara. Dicho algoritmo se basa en la suposición de que el único movimiento de la cámara es el que queremos eliminar; es decir, excepto por ligeros movimientos no deseados, la cámara está estática.
La idea del algoritmo es muy sencilla: 
Tomamos dos fotogramas consecutivos y buscamos una serie de puntos característicos mediante el algoritmo propuesto por Shi y Tomashi \cite{shiandtomasi}, usando la función: \lstinline|goodFeaturesToTrack|; después vemos a donde se han movido esos puntos en el siguiente fotograma mediante el algoritmo de flujo óptico de Lucas-Kanade.
Finalmente calculamos la homografía que lleva los puntos originales a donde hemos calculado que se han movido y le aplicamos la inversa de esa transformación al segundo fotograma para colocarlo donde debería estar.
\subsection{Algoritmo para encontrar puntos característicos}

Como ya hemos dicho, queremos encontrar una serie de puntos característicos que puedan ser localizados bien para poder encontrarlos en el siguiente fotograma. Es evidente que no vale cualquier punto, por ejemplo, si tenemos una pared completamente blanca, será muy difícil saber a donde se ha movido un punto cualquiera de la pared de un fotograma a otro. Por este motivo se introduce el concepto de buenos puntos característicos
(\textit{good features to track}).
Un primer acercamiento a definir qué puntos eran buenos lo dieron Chris Harris y Mike Stephens en ``A Combined Corner and Edge Detector" \cite{harris88}.
La idea básica es encontrar la diferencia de la intensidad de la imagen en un rectángulo centrado en $(x,y)$ . Esto se expresa con la función:
\begin{equation}
E(x,y)= \sum_{u,v} w(u,v) [I(u+x,v+y)-I(u,v)]^2
\end{equation}
Donde $w(x,y)$ es una función ventana, que asocia un peso a cada punto, e $I(x,y)$ es la intensidad de la imagen en cada punto.
Por tanto si E(x,y) es suficientemente grande, (x,y) será un buen punto.
Aplicando el desarrollo en serie de Taylor a $I(u+x,v+y)$ obtenemos:
\begin{equation}
I(u+x,v+y) \approx I(u, v) + I_x(u,v)x + I_y(u,v)y
\end{equation}
Donde $I_x$ e $I_y$ son las derivadas parciales en x e y respectivamente.
Esto nos lleva a la aproximación:
\begin{equation*}
E(x,y) \approx \sum_{u,v} w(u,v)[I_x(u,v)x + I_y(u,v)y]^2
\end{equation*}
que matricialmente se expresa como:
\begin{equation}
E(x,y) \approx 
\left( \begin{array}{cc}x & y \end{array} \right)
M 
\left( \begin{array}{c}x\\y \end{array} \right)
\end{equation}
siendo $M$:
\begin{equation*}
M = \sum_{u,v}w(u,v)
\left( \begin{array}{cc}
I_x^2 & I_xI_y\\
I_xI_y & I_y^2
 \end{array} \right)
\end{equation*}
Como nuestro objetivo era que E(x,y) fuese lo suficientemente grande, si estudiamos los autovalores de $M$, lo que querremos es que ambos sean suficientemente grandes.
Sean $\lambda_1$ y $\lambda_2$ dichos autovalores, entonces:
\begin{itemize}
\item Si  $ \lambda_1 \approx 0 $ y $\lambda_2 \approx 0 $ entonces el punto no tiene interés.
\item Si  $ \lambda_1 \approx 0 $ y $\lambda_2 $ es positivo y suficientemente grande entonces hemos encontrado un borde.
\item Si  $ \lambda_1 $ y $\lambda_2 $ son positivos y suficientemen grandes entonces hemos encontrado un punto de interés. Este punto es la intersección de dos bordes,por ello, estos puntos también son llamados esquinas .
\end{itemize}
Como función para valorar la calidad de los autovalores, Shi y Tomasi propusieron:
\begin{equation*}
R = min(\lambda_1, \lambda_2)
\end{equation*}
en lugar de la propuesta por Harris:
\begin{equation*}
R = \lambda_1 \lambda_2 - k(\lambda_1 + \lambda_2)^2
\end{equation*}
La función propuesta por Shi y Tomasi proporciona mejores resultados que la original.
De hacer todo esto se encarga la función \lstinline|cv2.goodFeaturesToTrack()|, que calcula las segundas derivadas utilizando el operador Sobel y luego calcula los autovalores.
Sus argumentos son:
\newline
\lstinline|cv2.goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]]) |
\begin{itemize}
\item \textit{image}: la imagen en la que queremos buscar los puntos (en escala de grises).
\item \textit{maxCorners}:  el máximo número de puntos característicos que queremos.
\item \textit{qualityLevel}: la calidad mínima que deben tener los puntos. Es un valor entre 0 y 1. Este parámetro es multiplicado por el mayor valor de calidad encontrado y sólo las esquinas que superen dicho valor serán devueltas.
\item \textit{minDistance}: distancia (euclídea) mínima que tiene que existir entre los puntos devueltos.
\item \textit{corners} (opcional): la lista de salida con los puntos encontrados.
\item \textit{mask} (opcional): región de interés en la que se buscarán los puntos.
\item \textit{blockSize} (opcional): tamaño del bloque para calcular la matriz $M$.
\item \textit{useHarrisDetector} (opcional): parámetro que indica si queremos usar el detector de Harris (en lugar de el de Shi y Tomasi).
\item \textit{k}: el parámetro para el detector de Harris.
\end{itemize}

\newpage
\subsection{Algoritmo de Lucas-Kanade}
El algortimo de Lucas-Kanade es un método que sirve para calcular el flujo óptico.
El flujo óptico es el patrón de movimiento aparente de los objetos entre un fotograma y el siguiente causado por un movimiento de la cámara o de los objetos. Se trata de un campo vectorial de dos dimensiones donde cada vector es un vector de desplazamiento de un punto entre ambos fotogramas.
Este algoritmo está basado en tres suposiciones:
\begin{enumerate}
\item Brillo constante: la apariencia de un píxel no cambia entre un frame y otro. Para una imagen en escala de grises esto es equivalente a que su brillo permanece constante
\item Los objetos no se mueven muy rápido.
\item Coherencia espacial: los píxeles vecinos tienen movimiento similar.
\end{enumerate}
El hecho de que el brillo no varie, es decir, que la intensidad de un píxel se mantiene nos lleva a la siguente igualdad:
\begin{equation}
I(x,y,t)=I(x+dx,y+dy,t+dt)
\end{equation}
Como hemos supuesto que el movimiento entre dos fotogramas es relativamente pequeño, podemos aproximar por desarrolo de Taylor la parte derecha de la igualdad anterior, de donde obtenemos:
\begin{equation}
\label{optflow}
I_x u + I_y v + I_t = 0
\end{equation}
 donde $u= \dfrac{dx}{dt}$ y $v= \dfrac{dy}{dt}$ y siendo $I_x$ e $I_y$ los gradientes de la imagen inicial y análogamente $I_t$ el gradiente sobre el tiempo.
La ecuación \ref{optflow} se conoce como la ecuación del flujo óptico y no puede ser resuelta para un solo punto ya que hay dos incógnitas.
Para resolverla utilizamos la suposición de la coherencia espacial, entonces podemos tomar una ventana en torno al punto y obtener un sistema de ecuaciones de la forma $Aw=-b$:
\begin{equation}
\left(
\begin{matrix}
I_x(p_1) & I_y(p_1)\\
I_x(p_2) & I_y(p_2)\\
\vdots & \vdots \\
I_x(p_n) & I_y(p_n)\\
\end{matrix}
\right)
\left(\begin{array}{c}u\\v\end{array}\right)
=
- \left(\begin{array}{c} I_t(p_1)\\I_t(p_2)\\ \vdots\\ I_t(p_n)\end{array}\right)
\end{equation}
El algoritmo de Lucas-Kanade obtiene una solución utlizando el ajuste por mínimos cuadrados. Resuelve el sistema:
\begin{equation*}
A^t Aw=A^t b
\end{equation*}
es decir:
\begin{equation*}
\left(
\begin{matrix}
\sum I_x I_x & \sum I_x I_y\\
\sum I_x I_y & \sum I_y I_y\\
\end{matrix}
\right)
\left(\begin{array}{c}u\\v\end{array}\right)
=
- \left(\begin{array}{c} \sum I_x I_t\\ \sum I_y I_t\end{array}\right)
\end{equation*}
y la solución a esta ecuación es $\left(\begin{array}{c}u\\v\end{array}\right) = (A^t A)^{-1} A^t b $  

El problema hasta ahora son las suposiciones de los movimientos pequeños y coherentes. Queremos usar una ventana grande para captar movimientos grandes pero esto puede romper dichas suposiciones. Para solventar esto aplicamos el algoritmo iterativamente a una pirámide de imágenes del fotograma. Primero se aplica el algoritmo a la imagen en la cima de la pirámide para seguir los movimiento grandes y luego según se va bajando en la pirámide se va refinando iterativamente el resultado partiendo del anterior. De esta manera los movimientos grandes se van convirtiendo en movimientos pequeños.
\newline
Este algoritmo lo realiza openCV mediante la función: \newline
\lstinline|cv2.calcOpticalFlowPyrLK(prevImg, nextImg, prevPts[, nextPts[, status[, err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]])|
$\rightarrow$  \lstinline|nextPts, status, err|
\newline

Donde:
\begin{itemize}
\item \lstinline|prevImg| y \lstinline|nextImg| son las dos imágenes entre las que queremos detectar movimiento y \lstinline|prevPts| los puntos que queremos seguir.
\item \lstinline|nextPts| es el vector de salida que contiene las nuevas posiciones de los puntos de entrada. Si se usa el argumento \lstinline|OPTFLOW_USE_INITIAL_FLOW|, este vector deberá tener el mismo tamaño que el de entrada.

\item \lstinline|status| es un vector de salida en el que cada elemento es 1 si el flujo para el punto correspondiente se ha encontrado y 0 en otro caso.

\item \lstinline|err| es un vector de salida que contiene en cada el elemento el error asociado a cada punto. El tipo de este error se puede fijar en el argumento \lstinline|flags|.  Si no se encuentra flujo el error no está definido.

\item \lstinline|winSize| es el tamaño de la ventana en cada nivel de la pirámide.


\item \lstinline|maxLevel| es el nivel máximo de niveles  de la pirámide a usar empezando desde 0. Si es 0 no se usará pirámide; si es 1, se usarán dos niveles, etc. 

\item \lstinline|criteria| es un parámetero que indica el criterio para terminar la búsqueda iterativa del algoritmo: después de un número máximo de iteraciones
o cuando la ventana de búsqueda se mueve menos que un épsilon dado.
\newpage
\item \lstinline|flags|:
	\begin{itemize}
    \item \lstinline|OPTFLOW_USE_INITIAL_FLOW| utiliza estimaciones iniciales, almacenadas en \lstinline|nextPst|. Si no, copia \lstinline|prevPts| a \lstinline|nextPts| y los considera la estimación inicial.
    
    \item \lstinline|OPTFLOW_LK_GET_MIN_EIGENVALS| utiliza los autovalores mínimos como medida de error. Si este parámetro no se indica se usará como medida de error la distancia $L_1$ entre los bloques del punto original y un punto movido, dividido por el número de píxeles en una ventana.
	\end{itemize}
\item \lstinline|minEigThreshold| –  El algortimo calcula el menor autovalor de la matriz de las ecuaciones del flujo. Si este valor dividido por el número de píxeles en una ventana es menor que \lstinline|minEigThreshold|, entonces el punto es filtrado y no se calcula su flujo.

\end{itemize}
\newpage

\subsection{Cálculo de la homografía}
En visión artificial se llama homografía a una transformación proyectiva de un plano a otro.
\newline
Tenemos una serie de puntos $q_1 \ldots q_n$ que van a parar a $q'_1 \ldots q'_n$. Denotaremos $q_i = (x_i,y_i)$ y $q'_i=(x'_i, y'_i)$,
Entonces podemos expresar la homografía como:
\begin{equation}
	\left( \begin{array}{c}
	x'_i\\
	y'_i\\
	1
	\end{array} \right)
	= sH
		\left( \begin{array}{c}
	x_i\\
	y_i\\
	1
	\end{array} \right)
\end{equation}
donde $s$ es un factor de escala y $H$ es la matriz de la transformación.
Para encontrar dicha matriz usamos la función \lstinline|cv.FindHomography()|
que en un principio devuelve dicha matriz minimizando el error:
\begin{equation*}
\sum_i \left( x'_i - \dfrac{h_{11}x_i + h_{12}y_i + h_{13}}{h_{31}x_i + h_{32}y_i + h_{33}} \right) ^2  
+ 
\left( y'_i - \dfrac{h_{21}x_i + h_{22}y_i + h_{23}}{h_{31}x_i + h_{32}y_i + h_{33}} \right) ^2 
\end{equation*}
Pero como no todos los puntos encajan en la transformación ya que puede haber errores u otros movimientos que no son los de la cámara utilizaremos un método robusto llamado RANSAC (RANdom SAmple Consensus). Se trata de un método iterativo para estimar los parámetros de un modelo matemático que contiene valores atípicos. La idea básica de este algoritmo es resolver varias veces el problema con distintos subconjuntos aleatorios de los puntos dados y luego tomar como solución particular la más cercana a la media/mediana de las soluciones.
\newpage
\subsection{Código}
\lstinputlisting[language=Python, frame= single]{estabilizacion.py}

\newpage
\section{Uso del acelerómetro para estabilizar}
Se trata de intentar utilizar los datos extra de los que disponemos, es decir, los que nos proporciona el acelerómetro del móvil para intentar estabilizar la imagen utilizando estos datos.
Para ello en primer lugar hacemos un programa para Android que se encargue de recoger estos datos, grabando el vídeo a la vez que registra los datos del acelerómetro y los guarda en un archivo junto con el momento exacto en el que se han registrado los datos.
Luego, un segundo programa en el ordenador procesa los archivos para intentar estabilizar la imagen: recoge los datos de aceleración en cada instante de media y aproxima el movimiento en dichos instantes. Luego calcula mediante interpolación cuanto se ha movido la cámara en el instante del fotograma y recoloca el fotograma de acuerdo con lo obtenido.

\subsection{Programa para Android}
A continuación se incluyen las partes más importantes del código de la aplicación de Android.
\begin{description}

\item [MainActivity] \hfill \\

\lstinputlisting[language=Java, frame= single]{Android/MainActivity.java}
\newpage
\item [Clase vcorderView] \hfill \\
A continuación incluimos el código de la clase \lstinline|vcorderView| que es la clase que se encarga de grabar de la cámara.
\lstinputlisting[language=Java, frame= single]{Android/vcorderView.java}

\item [Android manifest] \hfill \\
Ahora vemos el archivo \lstinline|AndroidManifest.xml| en el que cabe destacar las líneas en las que hemos indicado los permisos especiales que necesitamos (el uso de la cámara y de poder escribir en el almacenamiento del móvil).
\lstinputlisting[language=Xml, frame= single]{Android/AndroidManifest.xml}

\item [Activity layout] \hfill \\
Por último vemos el archivo XML que describe la interfaz de nuestra única actividad (y que tan solo es un marco y la zona en la que mostraremos la imagen mientras estemos grabando).
\lstinputlisting[language=Xml, frame= single]{Android/res/layout/activity_main.xml}
\end{description}
\newpage
\subsection{Programa de procesado de los datos}
\lstinputlisting[language=Python, frame= single]{estabilizacion_acelerometro.py}
\newpage

\subsection{Resultados}

\cleardoublepage
\addcontentsline{toc}{chapter}{Bibliografía}
\bibliographystyle{plain} % estilo de la bibliografía.
\bibliography{texto} % texto.bib es el fichero donde está salvada la bibliografía.


\end{document}
